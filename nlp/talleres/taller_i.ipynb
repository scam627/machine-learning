{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "taller-nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8MXF4Gx-yPf"
      },
      "source": [
        "**Corpus Description**\n",
        "\n",
        "Link reference [here](https://www.kaggle.com/rtatman/ubuntu-dialogue-corpus).\n",
        "\n",
        "**Content:**\n",
        "\n",
        "The new Ubuntu Dialogue Corpus consists of almost one million two-person conversations extracted from the Ubuntu chat logs, used to receive technical support for various Ubuntu-related problems. The conversations have an average of 8 turns each, with a minimum of 3 turns. All conversations are carried out in text form (not audio).\n",
        "\n",
        "The full dataset contains 930,000 dialogues and over 100,000,000 words and is available here. This dataset contains a sample of this dataset spread across .csv files. This dataset contains more than 269 million words of text, spread out over 26 million turns.\n",
        "\n",
        "* folder: The folder that a dialogue comes from. Each file contains dialogues from one folder .\n",
        "* dialogueID: An ID number for a specific dialogue. Dialogue ID’s are reused across folders.\n",
        "* date: A timestamp of the time this line of dialogue was sent.\n",
        "* from: The user who sent that line of dialogue.\n",
        "* to: The user to whom they were replying. On the first turn of a\n",
        "dialogue, this field is blank.\n",
        "* text: The text of that turn of dialogue, separated by double quotes (“). Line breaks (\\n) have been removed.\n",
        "\n",
        "**Files information**\n",
        "\n",
        "| lines  |  words  | characteres |      filename      | size  |\n",
        "|--------|---------|-------------|--------------------|-------|\n",
        "|9212878 |91660344 |  996253904  |dialogueText_196.csv|996,3MB|\n",
        "|16587831|166392849| 1799936480  |dialogueText_301.csv|1,8GB  |\n",
        "|1038325 |11035331 |  116070597  |dialogueText.csv    |116,1MB|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4SFYudIBvR2"
      },
      "source": [
        "**Context:**\n",
        "\n",
        "- Selected file: dialogueText.csv\n",
        "- Data divide by users with `scripts/process_file.py` and select first five heaviest files:\n",
        "\n",
        "```\n",
        "1. -rw-rw-r-- 1 stiven stiven 535351 abr 18 19:37 ActionParsnip.csv\n",
        "2. -rw-rw-r-- 1 stiven stiven 440231 abr 18 19:37 jrib.csv\n",
        "3. -rw-rw-r-- 1 stiven stiven 434792 abr 18 19:37 Dr_Willis.csv\n",
        "4. -rw-rw-r-- 1 stiven stiven 431646 abr 18 19:37 bazhang.csv\n",
        "5. -rw-rw-r-- 1 stiven stiven 429946 abr 18 19:37 Pici.csv\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U99zxVSlP35J"
      },
      "source": [
        "# Importations\n",
        "\n",
        "import nltk\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "nltk.download(\"book\")\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aGmJ2iwOM1t"
      },
      "source": [
        "1. **Normalization**: I just to trasform upper case to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MRvfTzLzHyM"
      },
      "source": [
        "text = open(\"./ActionParsnip.txt\", \"r\", encoding = \"UTF-8\")\n",
        "raw = text.read()\n",
        "type(raw)\n",
        "len(raw)\n",
        "raw = raw.lower()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zDQWh-GPGIa"
      },
      "source": [
        "2. **Segmentation**: Sentence tokenization via `sent_tokenize` of the nltk python module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg25oj0YPFuN",
        "outputId": "a657e6c2-f18e-45d6-ad4f-21cff50a5b04"
      },
      "source": [
        "sentences = sent_tokenize(raw)\n",
        "print(len(sentences))\n",
        "f = open(f'./frases.txt', 'w', encoding='utf-8')\n",
        "f.writelines(sentences)\n",
        "f.close()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJMWCweJRKbO"
      },
      "source": [
        "3. **Tokenization**: 18.979% of the tokens were deleted "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJk39ip-RKLB",
        "outputId": "42a347bb-8e79-4a1e-b41e-6920ad27a320"
      },
      "source": [
        "tokens = word_tokenize(raw)\n",
        "len(tokens) # -> 40613\n",
        "\n",
        "tokens = [ word for word in tokens if word.isalpha()]\n",
        "len(tokens) # -> 32905\n",
        "\n",
        "word_count = Counter(tokens)\n",
        "word_count.most_common(20)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 1682),\n",
              " ('you', 1472),\n",
              " ('to', 880),\n",
              " ('it', 731),\n",
              " ('a', 720),\n",
              " ('is', 650),\n",
              " ('in', 621),\n",
              " ('and', 618),\n",
              " ('can', 520),\n",
              " ('use', 411),\n",
              " ('will', 405),\n",
              " ('for', 320),\n",
              " ('sudo', 288),\n",
              " ('i', 266),\n",
              " ('run', 261),\n",
              " ('of', 256),\n",
              " ('then', 254),\n",
              " ('install', 251),\n",
              " ('as', 249),\n",
              " ('if', 245)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcERofJcGPvo"
      },
      "source": [
        "4. **Stop words elimination:** \n",
        "\n",
        "- 55.04% of the words were reduced with respect to tokens without alpha characters.\n",
        "- 44.5% of the words were reduced with respect to original tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XrifH25GPCG",
        "outputId": "8871ab99-220a-40a3-dbc3-a0c86bde0186"
      },
      "source": [
        "stopwords.words('english')[1:10]\n",
        "\n",
        "tokens = [ word for word in tokens if not word in stopwords.words('english')]\n",
        "len(tokens) # -> 18112"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTSUIpJiKFCz"
      },
      "source": [
        "5. **Stemming and lemmatization:** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZYWfJbbKEev",
        "outputId": "062e55e3-b23e-4f6d-e676-ec26b83e14d0"
      },
      "source": [
        "# STEMMING\n",
        "\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer \n",
        "porter_stem = PorterStemmer()\n",
        "snowball_stem = SnowballStemmer(\"english\")\n",
        "\n",
        "porter_tokens = [ porter_stem.stem(word) for word in tokens ]\n",
        "snowball_tokens = [ snowball_stem.stem(word) for word in tokens ]\n",
        "\n",
        "original_word_count = Counter(tokens)\n",
        "porter_word_count = Counter(porter_tokens)\n",
        "snowball_word_count = Counter(snowball_tokens)\n",
        "\n",
        "# Porter\n",
        "print('---------------------------(Porter)----------------------------------')\n",
        "percentage = (1 - len(porter_word_count) / len(original_word_count)) * 100\n",
        "print(f'{percentage}% of vocabulary size reduction')\n",
        "\n",
        "# Snowball\n",
        "print('--------------------------(Snowball)----------------------------------')\n",
        "percentage = (1 - len(snowball_word_count) / len(original_word_count)) * 100\n",
        "print(f'{percentage}% of vocabulary size reduction')\n",
        "\n",
        "# LEMMATIZATION\n",
        "\n",
        "from nltk import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "lemma_tokens = [ lemma.lemmatize(word, pos='v') for word in tokens ]\n",
        "lemma_tokens = [ lemma.lemmatize(word, pos='n') for word in tokens ]\n",
        "\n",
        "lemma_word_count = Counter(lemma_tokens)\n",
        "\n",
        "# Lemmatization\n",
        "print('--------------------------(Snowball)----------------------------------')\n",
        "percentage = (1 - len(lemma_word_count) / len(original_word_count)) * 100\n",
        "print(f'{percentage}% of vocabulary size reduction')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------(Porter)----------------------------------\n",
            "10.873408769448378% of vocabulary size reduction\n",
            "--------------------------(Snowball)----------------------------------\n",
            "10.590523338048087% of vocabulary size reduction\n",
            "--------------------------(Snowball)----------------------------------\n",
            "3.6951909476661937% of vocabulary size reduction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4drLXbdjSYKE"
      },
      "source": [
        "6. **Modifying sentences, collocations and text generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "aqQJnvRpSX-p",
        "outputId": "003faf99-9184-4e76-a7fc-fc3d260cd8b6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "sentences = [ f'<s> {sentence} </s>' for sentence in sentences]\n",
        "f = open(f'./frases.txt', 'w', encoding='utf-8')\n",
        "f.writelines(sentences)\n",
        "f.close()\n",
        " \n",
        "bigrams = nltk.collocations.BigramAssocMeasures()\n",
        "trigrams = nltk.collocations.TrigramAssocMeasures()\n",
        "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
        "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n",
        "\n",
        "# bigrams\n",
        "bigram_freq = bigramFinder.ngram_fd.items()\n",
        "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
        "# trigrams\n",
        "trigram_freq = trigramFinder.ngram_fd.items()\n",
        "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
        "\n",
        "bigramFreqTable\n",
        "# trigramFreqTable"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bigram</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>(run, sudo)</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>(sudo, lshw)</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>(sudo, install)</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>(test, iso)</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>(may, need)</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5685</th>\n",
              "      <td>(fine, times)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5686</th>\n",
              "      <td>(times, use)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5687</th>\n",
              "      <td>(use, itits)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5688</th>\n",
              "      <td>(itits, running)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15888</th>\n",
              "      <td>(lspci, one)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15889 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 bigram  freq\n",
              "118         (run, sudo)    68\n",
              "848        (sudo, lshw)    40\n",
              "172     (sudo, install)    34\n",
              "332         (test, iso)    33\n",
              "565         (may, need)    27\n",
              "...                 ...   ...\n",
              "5685      (fine, times)     1\n",
              "5686       (times, use)     1\n",
              "5687       (use, itits)     1\n",
              "5688   (itits, running)     1\n",
              "15888      (lspci, one)     1\n",
              "\n",
              "[15889 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyUs7vNlfFbL"
      },
      "source": [
        "split_sentences = [ sentence.split() for sentence in sentences ]\n",
        "bigrams_sentences = nltk.FreqDist([bigram for sentence in split_sentences for bigram in list(nltk.bigrams(sentence))])\n",
        "display(bigrams_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuGUZ6Zyf22n"
      },
      "source": [
        "Using an Stochastic process given in the class number five:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MjCsJbpfy6S"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def stochastic(freqdist, keys=None):\n",
        "    pivot = np.random.rand()\n",
        "    acc = 0.\n",
        "    if keys is None: \n",
        "        keys = freqdist.keys()\n",
        "    else:\n",
        "        pivot = pivot * np.sum([freqdist.freq(k) for k in keys])\n",
        "        \n",
        "    for key in keys:\n",
        "        acc = acc + freqdist.freq(key) \n",
        "        if pivot < acc: return key\n",
        "\n",
        "def keys(freqdist, start: tuple):\n",
        "    assert(type(start) is tuple)\n",
        "    return [key for key in freqdist.keys() if start == key[:len(start)]]\n",
        "\n",
        "def bigram_generator(prev= None, max_length=100):\n",
        "    sentence = prev.upper().split() if prev else list()\n",
        "    prev = (sentence[-1],) if prev else ('<s>', )\n",
        "    for i in range(max_length):\n",
        "        keys_list = keys(bigrams_sentences, prev)\n",
        "        curr = stochastic(bigrams_sentences, keys_list)[-1]\n",
        "        if curr == '</s>': break\n",
        "        sentence.append(curr)\n",
        "        prev = (curr, )\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "bigram_generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeR3dBKci_GY"
      },
      "source": [
        "6. Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alik0pPNmYXa",
        "outputId": "8961d9ba-a0c6-418c-9ae5-ba59f43cfcf3"
      },
      "source": [
        "# This program computes the \"distance\" between two text files\n",
        "# as the angle between their word frequency vectors (in radians).\n",
        "#\n",
        "# For each input file, a word-frequency vector is computed as follows:\n",
        "#    (1) the specified file is read in it is converted into a list of alphanumeric \"words\"\n",
        "#    (2) for each word, its frequency of occurrence is determined\n",
        "#    (3) the word/frequency lists are sorted into order alphabetically\n",
        "#\n",
        "# The \"distance\" between two vectors is the angle between them.\n",
        "# If x = (x1, x2, ..., xn) is the first vector (xi = freq of word i)\n",
        "# and y = (y1, y2, ..., yn) is the second vector,\n",
        "# then the angle between them is defined as:\n",
        "#    d(x,y) = arccos(inner_product(x,y) / (norm(x)*norm(y)))\n",
        "# where:\n",
        "#    inner_product(x,y) = x1*y1 + x2*y2 + ... xn*yn\n",
        "#    norm(x) = sqrt(inner_product(x,x))\n",
        "\n",
        "import math\n",
        "    # math.acos(x) is the arccosine of x.\n",
        "    # math.sqrt(x) is the square root of x.\n",
        "\n",
        "import string\n",
        "\n",
        "import sys\n",
        "\n",
        "#####################################################################\n",
        "# Operation 1: read a text file and apply natural language process ##\n",
        "#####################################################################\n",
        "def read_file(filename):\n",
        "    \"\"\" \n",
        "    Read the text file with the given filename;\n",
        "    return a list of the lines of text in the file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = open(filename, \"r\", encoding = \"UTF-8\")\n",
        "        raw = text.read()\n",
        "        raw = raw.lower()\n",
        "        tokens = word_tokenize(raw)\n",
        "        tokens = [ word for word in tokens if word.isalpha()]\n",
        "        tokens = [ word for word in tokens if not word in stopwords.words('english')]\n",
        "        \n",
        "        from nltk.stem import PorterStemmer, SnowballStemmer \n",
        "        porter_stem = PorterStemmer()\n",
        "        snowball_stem = SnowballStemmer(\"english\")\n",
        "        porter_tokens = [ porter_stem.stem(word) for word in tokens ]\n",
        "        snowball_tokens = [ snowball_stem.stem(word) for word in tokens ]\n",
        "        \n",
        "        from nltk import WordNetLemmatizer\n",
        "        lemma = WordNetLemmatizer()\n",
        "        lemma_tokens = [ lemma.lemmatize(word, pos='v') for word in tokens ]\n",
        "        lemma_tokens = [ lemma.lemmatize(word, pos='n') for word in tokens ]\n",
        "        \n",
        "        return tokens, porter_tokens, snowball_tokens, lemma_tokens\n",
        "    except IOError:\n",
        "        print(\"Error opening or reading input file: \",filename)\n",
        "        sys.exit()\n",
        "\n",
        "##############################################\n",
        "# Operation 2: count frequency of each word ##\n",
        "##############################################\n",
        "def count_frequency(word_list):\n",
        "    \"\"\"\n",
        "    Return a dictionary mapping words to frequency.\n",
        "    \"\"\"\n",
        "    D = {}\n",
        "    for new_word in word_list:\n",
        "        if new_word in D:\n",
        "            D[new_word] = D[new_word]+1\n",
        "        else:\n",
        "            D[new_word] = 1\n",
        "    return D\n",
        "\n",
        "#############################################\n",
        "## compute word frequencies for input file ##\n",
        "#############################################\n",
        "def word_frequencies_for_file(filename):\n",
        "    \"\"\"\n",
        "    Return dictionary of (word,frequency) pairs for the given file.\n",
        "    \"\"\"\n",
        "\n",
        "    word_list, stem_porter_word , stem_snowball_word, lem_word = read_file(filename)\n",
        "    \n",
        "    # we can change tokens by stemm words or lem words\n",
        "    freq_mapping = count_frequency(word_list)\n",
        "\n",
        "    print(\"File\",filename,\":\")\n",
        "    print(len(word_list),\"words,\")\n",
        "    print(len(freq_mapping),\"distinct words\")\n",
        "\n",
        "    return freq_mapping\n",
        "\n",
        "def inner_product(D1,D2):\n",
        "    \"\"\"\n",
        "    Inner product between two vectors, where vectors\n",
        "    are represented as dictionaries of (word,freq) pairs.\n",
        "\n",
        "    Example: inner_product({\"and\":3,\"of\":2,\"the\":5},\n",
        "                           {\"and\":4,\"in\":1,\"of\":1,\"this\":2}) = 14.0 \n",
        "    \"\"\"\n",
        "    sum = 0.0\n",
        "    for key in D1:\n",
        "        if key in D2:\n",
        "            sum += D1[key] * D2[key]\n",
        "    return sum\n",
        "\n",
        "def vector_angle(D1,D2):\n",
        "    \"\"\"\n",
        "    The input is a list of (word,freq) pairs, sorted alphabetically.\n",
        "\n",
        "    Return the angle between these two vectors.\n",
        "    \"\"\"\n",
        "    numerator = inner_product(D1,D2)\n",
        "    denominator = math.sqrt(inner_product(D1,D1)*inner_product(D2,D2))\n",
        "    return math.acos(numerator/denominator)\n",
        "\n",
        "def main():\n",
        "    filename_1 = 'ActionParsnip.txt'\n",
        "    filename_2 = 'Pici.txt'\n",
        "    sorted_word_list_1 = word_frequencies_for_file(filename_1)\n",
        "    sorted_word_list_2 = word_frequencies_for_file(filename_2)\n",
        "    distance = vector_angle(sorted_word_list_1,sorted_word_list_2)\n",
        "    print(\"The distance between the documents is: %0.6f (radians)\"%distance)\n",
        "\n",
        "main()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ActionParsnip.txt :\n",
            "18112 words,\n",
            "5656 distinct words\n",
            "File Pici.txt :\n",
            "13528 words,\n",
            "3238 distinct words\n",
            "The distance between the documents is: 0.964608 (radians)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}